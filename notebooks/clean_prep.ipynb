{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ffc43c1-d0dc-4c61-b5dc-7ad0aafdd504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ../data/processed/game/games_wide_2016_regular.parquet -> games_2016 (alias games_16, df_2016), shape=(832, 196)\n",
      "Loaded ../data/processed/game/games_wide_2017_regular.parquet -> games_2017 (alias games_17, df_2017), shape=(834, 196)\n",
      "Loaded ../data/processed/game/games_wide_2018_regular.parquet -> games_2018 (alias games_18, df_2018), shape=(845, 196)\n",
      "Loaded ../data/processed/game/games_wide_2019_regular.parquet -> games_2019 (alias games_19, df_2019), shape=(848, 196)\n",
      "Loaded ../data/processed/game/games_wide_2020_regular.parquet -> games_2020 (alias games_20, df_2020), shape=(542, 196)\n",
      "Loaded ../data/processed/game/games_wide_2021_regular.parquet -> games_2021 (alias games_21, df_2021), shape=(849, 196)\n",
      "Loaded ../data/processed/game/games_wide_2022_regular.parquet -> games_2022 (alias games_22, df_2022), shape=(854, 196)\n",
      "Loaded ../data/processed/game/games_wide_2023_regular.parquet -> games_2023 (alias games_23, df_2023), shape=(868, 196)\n",
      "Loaded ../data/processed/game/games_wide_2024_regular.parquet -> games_2024 (alias games_24, df_2024), shape=(874, 196)\n",
      "Loaded ../data/processed/game/games_wide_2023_regular.parquet -> games_2023 (alias games_23, df_2023), shape=(868, 196)\n",
      "Loaded ../data/processed/game/games_wide_2024_regular.parquet -> games_2024 (alias games_24, df_2024), shape=(874, 196)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# directory containing game parquet files (relative to this notebook)\n",
    "path = Path('..') / 'data' / 'processed' / 'game'\n",
    "files = sorted(path.glob('*.parquet'))\n",
    "# load exactly one DataFrame per season (keep the first file encountered for each year)\n",
    "games = {}  # maps YYYY -> DataFrame\n",
    "for f in files:\n",
    "    # read parquet first so any I/O/parquet-engine errors surface immediately\n",
    "    df = pd.read_parquet(f)\n",
    "\n",
    "    # --- reorder columns: week, team names, team points, home_*, away_*, rest ---\n",
    "    week_col = None\n",
    "    for candidate in ('week', 'week_num', 'week_number'):\n",
    "        if candidate in df.columns:\n",
    "            week_col = candidate\n",
    "            break\n",
    "    team_cols = [c for c in ('home_team', 'away_team') if c in df.columns]\n",
    "    points_cols = [c for c in ('home_points', 'away_points') if c in df.columns]\n",
    "    home_pref = [c for c in df.columns if c.startswith('home_') and c not in points_cols and c not in team_cols]\n",
    "    away_pref = [c for c in df.columns if c.startswith('away_') and c not in points_cols and c not in team_cols]\n",
    "    used = set()\n",
    "    if week_col:\n",
    "        used.add(week_col)\n",
    "    used.update(team_cols)\n",
    "    used.update(points_cols)\n",
    "    used.update(home_pref)\n",
    "    used.update(away_pref)\n",
    "    remaining = [c for c in df.columns if c not in used]\n",
    "    new_order = []\n",
    "    if week_col:\n",
    "        new_order.append(week_col)\n",
    "    new_order.extend(team_cols)\n",
    "    new_order.extend(points_cols)\n",
    "    new_order.extend(home_pref)\n",
    "    new_order.extend(away_pref)\n",
    "    new_order.extend(remaining)\n",
    "    new_order = [c for c in new_order if c in df.columns]\n",
    "    df = df.loc[:, new_order]\n",
    "    # --- end reorder ---\n",
    "\n",
    "    fname = f.name\n",
    "    m = re.search(r'(19|20)\\d{2}', fname)\n",
    "    if m:\n",
    "        year = m.group(0)\n",
    "    else:\n",
    "        # fallback: use filename without suffix\n",
    "        year = fname.rsplit('.parquet', 1)[0]\n",
    "    # normalize to a 4-digit year string when possible\n",
    "    year_str = year if (isinstance(year, str) and len(str(year)) == 4 and str(year).isdigit()) else str(year)\n",
    "    # if we've already loaded a DataFrame for this season, skip further files\n",
    "    if year_str in games:\n",
    "        # skip duplicates for the same season\n",
    "        continue\n",
    "    # register the DataFrame for this season\n",
    "    games[year_str] = df\n",
    "    # expose short-name globals: games_YY (e.g. games_16) and games_YYYY\n",
    "    short = year_str[-2:] if year_str.isdigit() and len(year_str) == 4 else year_str\n",
    "    globals()[f'games_{year_str}'] = df\n",
    "    globals()[f'games_{short}'] = df\n",
    "    # backward compatibility: also expose df_YYYY and df_YY aliases\n",
    "    globals()[f'df_{year_str}'] = df\n",
    "    globals()[f'df_{short}'] = df\n",
    "    print(f'Loaded {f} -> games_{year_str} (alias games_{short}, df_{year_str}), shape={df.shape}')\n",
    "# convenience alias: dfs points to the per-season mapping we just built\n",
    "dfs = games\n",
    "# use dfs['2017'] or games_17 / df_17 as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad71b3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(832, 166)\n",
      "(834, 166)\n",
      "(845, 166)\n",
      "(848, 166)\n",
      "(542, 166)\n",
      "(849, 166)\n",
      "(854, 166)\n",
      "(868, 166)\n",
      "(874, 166)\n"
     ]
    }
   ],
   "source": [
    "drop_cols = ['home_team_x','away_team_x','home_team_y','away_team_y','away_interceptionTDs','home_interceptonTDs','home_puntReturnYards','away_puntReturnYards','away_kickReturnYards','home_kickReturnYards','home_kickReturnTDs','away_kickReturnTDs','favorite','season', 'season_type','start_date','home_conference','venue','neutral_site','conference_game','away_conference','home_KickReturns','away_kickReturns','home_puntReturns','home_puntReturnTDs','away_puntReturnTDs','home_interceptionYards','away_interceptionYards','home_interceptionTDs','away_interceptonTDs','away_puntReturns','home_puntReturnTDs','away_puntReturnTDs','home_team_conference','away_team_conference']\n",
    "dfs = [games_2016, games_2017, games_2018, games_2019, games_2020, games_2021, games_2022, games_2023, games_2024]\n",
    "for df in dfs:\n",
    "    df.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d2eb50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: df | rows=874 cols=166\n",
      "Total null values: 1950\n",
      "Top 20 columns by null count:\n",
      "                        null_count   null_pct\n",
      "away_passesIntercepted         434  49.656751\n",
      "home_passesIntercepted         358  40.961098\n",
      "away_totalFumbles              184  21.052632\n",
      "home_kickReturns               178  20.366133\n",
      "home_totalFumbles              172  19.679634\n",
      "away_kickingPoints              22   2.517162\n",
      "total                           17   1.945080\n",
      "spread                          16   1.830664\n",
      "home_tacklesForLoss              7   0.800915\n",
      "home_qbHurries                   7   0.800915\n",
      "home_sacks                       7   0.800915\n",
      "home_tackles                     7   0.800915\n",
      "home_passesDeflected             7   0.800915\n",
      "home_defensiveTDs                7   0.800915\n",
      "home_kickingPoints               6   0.686499\n",
      "away_passesDeflected             6   0.686499\n",
      "away_tacklesForLoss              6   0.686499\n",
      "away_tackles                     6   0.686499\n",
      "away_sacks                       6   0.686499\n",
      "away_qbHurries                   6   0.686499\n"
     ]
    }
   ],
   "source": [
    "# Count nulls per column and overall for a target DataFrame\n",
    "def count_nulls(df):\n",
    "    \"\"\"Return (summary_df, total_nulls). summary_df has columns: null_count, null_pct.\"\"\"\n",
    "    nulls = df.isna().sum()\n",
    "    total_nulls = int(nulls.sum())\n",
    "    pct = (nulls / len(df)) * 100 if len(df) > 0 else 0\n",
    "    summary = pd.DataFrame({'null_count': nulls.astype(int), 'null_pct': pct})\n",
    "    summary = summary.sort_values('null_count', ascending=False)\n",
    "    return summary, total_nulls\n",
    "\n",
    "# Auto-detect a sensible target dataframe (adjust candidates if you use other names)\n",
    "for candidate in ('df_a', 'df', 'df_16', 'df_2016', 'df_2024', 'df_24'):\n",
    "    if candidate in globals():\n",
    "        target_name = candidate\n",
    "        break\n",
    "else:\n",
    "    raise NameError('No target dataframe found. Define `df` or `df_a` or df_YYYY beforehand.')\n",
    "\n",
    "target_df = globals()[target_name]\n",
    "summary, total_nulls = count_nulls(target_df)\n",
    "print(f'Target: {target_name} | rows={len(target_df)} cols={target_df.shape[1]}')\n",
    "print(f'Total null values: {total_nulls}')\n",
    "print('Top 20 columns by null count:')\n",
    "print(summary.head(20).to_string())\n",
    "\n",
    "# Optionally save summary to CSV/JSON for later inspection (uncomment to enable)\n",
    "# out_csv = Path('..') / 'data' / 'processed' / f'nulls_{target_name}.csv'\n",
    "# summary.to_csv(out_csv)\n",
    "# print('Saved null-summary ->', out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b20fb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: nulls before=0, after=0, shape=(832, 196)\n",
      "2017: nulls before=0, after=0, shape=(834, 196)\n",
      "2018: nulls before=0, after=0, shape=(845, 196)\n",
      "2019: nulls before=0, after=0, shape=(848, 196)\n",
      "2020: nulls before=0, after=0, shape=(542, 196)\n",
      "2021: nulls before=0, after=0, shape=(849, 196)\n",
      "2022: nulls before=0, after=0, shape=(854, 196)\n",
      "2023: nulls before=0, after=0, shape=(868, 196)\n",
      "2024: nulls before=0, after=0, shape=(874, 196)\n",
      "df_2016: nulls before=0, after=0, shape=(832, 196)\n",
      "df_16: nulls before=0, after=0, shape=(832, 196)\n",
      "df_2017: nulls before=0, after=0, shape=(834, 196)\n",
      "df_17: nulls before=0, after=0, shape=(834, 196)\n",
      "df_2018: nulls before=0, after=0, shape=(845, 196)\n",
      "df_18: nulls before=0, after=0, shape=(845, 196)\n",
      "df_2019: nulls before=0, after=0, shape=(848, 196)\n",
      "df_19: nulls before=0, after=0, shape=(848, 196)\n",
      "df_2020: nulls before=0, after=0, shape=(542, 196)\n",
      "df_20: nulls before=0, after=0, shape=(542, 196)\n",
      "df_2021: nulls before=0, after=0, shape=(849, 196)\n",
      "df_21: nulls before=0, after=0, shape=(849, 196)\n",
      "df_2022: nulls before=0, after=0, shape=(854, 196)\n",
      "df_22: nulls before=0, after=0, shape=(854, 196)\n",
      "df_2023: nulls before=0, after=0, shape=(868, 196)\n",
      "df_23: nulls before=0, after=0, shape=(868, 196)\n",
      "df_2024: nulls before=0, after=0, shape=(874, 196)\n",
      "df_24: nulls before=0, after=0, shape=(874, 196)\n",
      "Completed filling nulls; set nulls_filled=True\n"
     ]
    }
   ],
   "source": [
    "# Fill all nulls (NA/NaN) with 0 for detected DataFrames\n",
    "import re\n",
    "def fill_all_nulls():\n",
    "    \"\"\"Detect DataFrame objects in common locations (dfs dict/list, df_YYYY names)\n",
    "    and replace all missing values with 0 in-place. Returns a list of report tuples.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    # check `dfs` if present (can be dict or list)\n",
    "    if 'dfs' in globals():\n",
    "        obj = globals()['dfs']\n",
    "        if isinstance(obj, dict):\n",
    "            for k,v in obj.items():\n",
    "                if isinstance(v, pd.DataFrame):\n",
    "                    candidates.append((k,v))\n",
    "        elif isinstance(obj, list):\n",
    "            for i,v in enumerate(obj):\n",
    "                if isinstance(v, pd.DataFrame):\n",
    "                    candidates.append((f'dfs[{i}]', v))\n",
    "    # discover globals named df_YY or df_YYYY\n",
    "    for name, val in list(globals().items()):\n",
    "        if re.match(r'^df_\\d{2,4}$', name) and isinstance(val, pd.DataFrame):\n",
    "            # avoid duplicate entries\n",
    "            if not any(name == n for n,_ in candidates):\n",
    "                candidates.append((name, val))\n",
    "    if not candidates:\n",
    "        raise NameError('No DataFrame candidates found to fill nulls. Define `df`, `df_a`, or `dfs` first.')\n",
    "    reports = []\n",
    "    for name, df in candidates:\n",
    "        before = int(df.isna().sum().sum())\n",
    "        # perform in-place fill; this will replace NA in all dtypes with 0\n",
    "        df.fillna(0, inplace=True)\n",
    "        after = int(df.isna().sum().sum())\n",
    "        reports.append((name, before, after, df.shape))\n",
    "    return reports\n",
    "\n",
    "# Run the operation and print a compact report\n",
    "reports = fill_all_nulls()\n",
    "for name, before, after, shape in reports:\n",
    "    print(f'{name}: nulls before={before}, after={after}, shape={shape}')\n",
    "\n",
    "# signal completion to the notebook session\n",
    "globals()['nulls_filled'] = True\n",
    "print('Completed filling nulls; set nulls_filled=True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f1b4aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files:\n",
      " - ../data/processed/structured/games_2016.parquet\n",
      " - ../data/processed/structured/games_2017.parquet\n",
      " - ../data/processed/structured/games_2018.parquet\n",
      " - ../data/processed/structured/games_2019.parquet\n",
      " - ../data/processed/structured/games_2020.parquet\n",
      " - ../data/processed/structured/games_2021.parquet\n",
      " - ../data/processed/structured/games_2022.parquet\n",
      " - ../data/processed/structured/games_2023.parquet\n",
      " - ../data/processed/structured/games_2024.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save ONE DataFrame per season as games_YYYY.parquet into ../data/processed/structured\n",
    "from pathlib import Path\n",
    "import re\n",
    "out_dir = Path('..') / 'data' / 'processed' / 'structured'\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "saved = []\n",
    "\n",
    "def _sanitize_df_for_parquet(df):\n",
    "    \"\"\"Return a copy of df with column names as str and decode/normalize problematic values.\n",
    "    Specifically coerce `home_team` / `away_team` to str and ensure object columns are\n",
    "    decoded from bytes and cast to string to avoid pyarrow binary/inference issues.\n",
    "    \"\"\"\n",
    "    d = df.copy()\n",
    "    # ensure column names are strings\n",
    "    d.columns = [str(c) for c in d.columns]\n",
    "\n",
    "    # Coerce known problematic team columns to string (handles ints, bytes, NaN)\n",
    "    for col in ('home_team', 'away_team'):\n",
    "        if col in d.columns:\n",
    "            d[col] = d[col].where(d[col].notnull(), '').astype(str)\n",
    "\n",
    "    # decode bytes/bytearray in object columns and cast to str (preserve empties)\n",
    "    obj_cols = d.select_dtypes(include=['object']).columns.tolist()\n",
    "    if obj_cols:\n",
    "        for c in obj_cols:\n",
    "            def _maybe_decode(x):\n",
    "                if isinstance(x, (bytes, bytearray)):\n",
    "                    try:\n",
    "                        return x.decode('utf-8')\n",
    "                    except Exception:\n",
    "                        return str(x)\n",
    "                return x\n",
    "            d[c] = d[c].apply(_maybe_decode)\n",
    "            # replace NaN with empty string then cast to str to avoid mixed-type columns\n",
    "            d[c] = d[c].where(d[c].notnull(), '').astype(str)\n",
    "    return d\n",
    "\n",
    "# If `dfs` is a dict mapping year->df, prefer that (common earlier in this notebook)\n",
    "if 'dfs' in globals():\n",
    "    container = globals()['dfs']\n",
    "    if isinstance(container, dict):\n",
    "        for key, df in container.items():\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                continue\n",
    "            k = str(key)\n",
    "            m = re.search(r'(19|20)\\d{2}', k)\n",
    "            year = m.group(0) if m else re.sub(r'[^0-9]', '', k) or k\n",
    "            out_path = out_dir / f'games_{year}.parquet'\n",
    "            try:\n",
    "                d = _sanitize_df_for_parquet(df)\n",
    "                # write parquet (pyarrow/fastparquet); index not required\n",
    "                d.to_parquet(out_path.as_posix(), index=False)\n",
    "                saved.append(str(out_path))\n",
    "            except Exception as e:\n",
    "                print(f'Parquet write failed for {year}: {e!s}')\n",
    "    elif isinstance(container, list):\n",
    "        # if dfs is a list, attempt to save using games_YYYY globals instead (fallback below)\n",
    "        pass\n",
    "# Fallback: look for globals named games_YYYY or games_YY and save each once\n",
    "seen = set()\n",
    "for name, val in list(globals().items()):\n",
    "    if re.match(r'^(games|df)_(19|20)\\d{2}$', name) and isinstance(val, pd.DataFrame):\n",
    "        year = name.split('_',1)[1]\n",
    "        if year in seen:\n",
    "            continue\n",
    "        out_path = out_dir / f'games_{year}.parquet'\n",
    "        try:\n",
    "            d = _sanitize_df_for_parquet(val)\n",
    "            d.to_parquet(out_path.as_posix(), index=False)\n",
    "            saved.append(str(out_path))\n",
    "            seen.add(year)\n",
    "        except Exception as e:\n",
    "            print(f'Parquet write failed for {year}: {e!s}')\n",
    "# Deduplicate saved list and report\n",
    "saved = sorted(set(saved))\n",
    "if saved:\n",
    "    print('Saved files:')\n",
    "    for p in saved:\n",
    "        print(' -', p)\n",
    "else:\n",
    "    print('No season DataFrames found to save. Ensure `dfs` dict or `games_YYYY` globals exist and rerun previous cells.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117086be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                UAB\n",
       "1      Southern Miss\n",
       "2           Marshall\n",
       "3               Army\n",
       "4        Texas State\n",
       "           ...      \n",
       "537       Penn State\n",
       "538             UCLA\n",
       "539       Cincinnati\n",
       "540          Florida\n",
       "541     Oregon State\n",
       "Name: home_team, Length: 542, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games_20['home_team']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01dd8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "who_covers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
